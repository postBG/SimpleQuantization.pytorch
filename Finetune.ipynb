{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Finetune.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"OdUzwkisrc76","colab_type":"code","outputId":"15366f43-13ea-48cd-8462-9caf727e5507","executionInfo":{"status":"ok","timestamp":1570522040958,"user_tz":-540,"elapsed":555969,"user":{"displayName":"박은혁","photoUrl":"","userId":"15132345639082720119"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["'''Train CIFAR10 with PyTorch.'''\n","import torch\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","import torch.backends.cudnn as cudnn\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","drive_path = \"drive/My Drive\" # do not modify this line\n","relative_path = \"/practice\"   # set to your relative path \n","\n","base_path = drive_path + relative_path\n","\n","import sys\n","sys.path.append(base_path)\n","\n","from train_test import train, test\n","\n","LR = 0.01\n","EPOCH = 5\n","\n","# Data\n","print('==> Preparing data')\n","from dataset import cifar10_dataset\n","trainloader, testloader = cifar10_dataset(base_path + \"/data\")\n","\n","# Model\n","print('==> Building model')\n","from resnet_quant import ResNet18\n","model = ResNet18()\n","model.load_state_dict(torch.load(base_path + \"/train_best.pth\"))\n","\n","if torch.cuda.is_available():\n","    model.cuda()\n","    model = torch.nn.DataParallel(model)\n","    cudnn.benchmark = True\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","print('==> Full-precision model accuracy')\n","from quant_op import Q_ReLU, Q_Conv2d, Q_Linear\n","test(model, testloader, criterion)\n","\n","for name, module in model.named_modules():\n","    if isinstance(module, Q_ReLU):\n","        module.n_lv = 8\n","        module.bound = 1\n","    \n","    if isinstance(module, (Q_Conv2d, Q_Linear)):\n","        module.n_lv = 8\n","        module.ratio = 0.5\n","\n","print('==> Quantized model accuracy')\n","from quant_op import Q_ReLU, Q_Conv2d, Q_Linear\n","test(model, testloader, criterion)\n","\n","best_acc = 0  # best test accuracy\n","start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n","\n","optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n","scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCH, last_epoch=start_epoch-1)\n","\n","for epoch in range(start_epoch, start_epoch + EPOCH):\n","    scheduler.step()\n","    train(model, trainloader, criterion, optimizer, epoch)\n","    acc = test(model, testloader, criterion)\n","\n","    if acc > best_acc:\n","        best_acc = acc\n","        torch.save(model.module.state_dict(), base_path +  \"quant_best.pth\")\n","\n","print('==> Fine-tuned model accuracy')\n","from quant_op import Q_ReLU, Q_Conv2d, Q_Linear\n","test(model, testloader, criterion)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","==> Preparing data\n","Files already downloaded and verified\n","Files already downloaded and verified\n","==> Building model\n","==> Full-precision model accuracy\n","Test: [10/100]\tTime 0.063 (0.111)\tLoss 0.1693 (0.2120)\tPrec@1 95.000 (94.300)\n","Test: [20/100]\tTime 0.060 (0.085)\tLoss 0.3840 (0.2106)\tPrec@1 93.000 (94.650)\n","Test: [30/100]\tTime 0.060 (0.077)\tLoss 0.1171 (0.2190)\tPrec@1 96.000 (94.667)\n","Test: [40/100]\tTime 0.060 (0.073)\tLoss 0.1930 (0.2174)\tPrec@1 97.000 (94.750)\n","Test: [50/100]\tTime 0.061 (0.070)\tLoss 0.3337 (0.2224)\tPrec@1 94.000 (94.660)\n","Test: [60/100]\tTime 0.064 (0.069)\tLoss 0.1739 (0.2162)\tPrec@1 96.000 (94.817)\n","Test: [70/100]\tTime 0.060 (0.068)\tLoss 0.1721 (0.2082)\tPrec@1 96.000 (95.014)\n","Test: [80/100]\tTime 0.060 (0.067)\tLoss 0.0803 (0.2097)\tPrec@1 96.000 (95.013)\n","Test: [90/100]\tTime 0.059 (0.066)\tLoss 0.2229 (0.2079)\tPrec@1 94.000 (94.989)\n","Test: [100/100]\tTime 0.060 (0.065)\tLoss 0.0745 (0.2060)\tPrec@1 97.000 (94.980)\n"," * Prec@1 94.980\n","==> Quantized model accuracy\n","Test: [10/100]\tTime 0.089 (0.136)\tLoss 0.6349 (0.6542)\tPrec@1 89.000 (84.400)\n","Test: [20/100]\tTime 0.095 (0.113)\tLoss 0.7419 (0.6521)\tPrec@1 82.000 (84.550)\n","Test: [30/100]\tTime 0.090 (0.105)\tLoss 0.7725 (0.6547)\tPrec@1 86.000 (84.867)\n","Test: [40/100]\tTime 0.087 (0.101)\tLoss 0.7525 (0.6527)\tPrec@1 84.000 (84.750)\n","Test: [50/100]\tTime 0.088 (0.098)\tLoss 0.9399 (0.6533)\tPrec@1 76.000 (84.500)\n","Test: [60/100]\tTime 0.089 (0.097)\tLoss 0.8306 (0.6562)\tPrec@1 85.000 (84.733)\n","Test: [70/100]\tTime 0.089 (0.095)\tLoss 0.6954 (0.6531)\tPrec@1 81.000 (84.714)\n","Test: [80/100]\tTime 0.086 (0.094)\tLoss 0.5551 (0.6545)\tPrec@1 83.000 (84.625)\n","Test: [90/100]\tTime 0.086 (0.094)\tLoss 0.5673 (0.6542)\tPrec@1 84.000 (84.533)\n","Test: [100/100]\tTime 0.086 (0.093)\tLoss 0.4928 (0.6517)\tPrec@1 87.000 (84.480)\n"," * Prec@1 84.480\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0: [10/196]\tTime 0.491 (0.532)\tData 0.001 (0.067)\tLoss 0.0612 (0.0557)\tPrec@1 98.047 (98.398)\n","Epoch 0: [20/196]\tTime 0.489 (0.508)\tData 0.002 (0.034)\tLoss 0.0799 (0.0504)\tPrec@1 97.656 (98.359)\n","Epoch 0: [30/196]\tTime 0.488 (0.501)\tData 0.001 (0.023)\tLoss 0.1054 (0.0543)\tPrec@1 96.484 (98.333)\n","Epoch 0: [40/196]\tTime 0.491 (0.497)\tData 0.002 (0.018)\tLoss 0.0430 (0.0508)\tPrec@1 98.438 (98.428)\n","Epoch 0: [50/196]\tTime 0.492 (0.495)\tData 0.002 (0.015)\tLoss 0.0709 (0.0502)\tPrec@1 98.047 (98.438)\n","Epoch 0: [60/196]\tTime 0.485 (0.493)\tData 0.002 (0.012)\tLoss 0.0361 (0.0506)\tPrec@1 99.219 (98.405)\n","Epoch 0: [70/196]\tTime 0.486 (0.492)\tData 0.002 (0.011)\tLoss 0.0346 (0.0492)\tPrec@1 99.219 (98.449)\n","Epoch 0: [80/196]\tTime 0.489 (0.491)\tData 0.001 (0.010)\tLoss 0.0596 (0.0487)\tPrec@1 98.047 (98.447)\n","Epoch 0: [90/196]\tTime 0.489 (0.491)\tData 0.001 (0.009)\tLoss 0.0604 (0.0489)\tPrec@1 97.656 (98.416)\n","Epoch 0: [100/196]\tTime 0.489 (0.490)\tData 0.002 (0.008)\tLoss 0.0721 (0.0503)\tPrec@1 97.656 (98.359)\n","Epoch 0: [110/196]\tTime 0.490 (0.490)\tData 0.001 (0.008)\tLoss 0.0429 (0.0503)\tPrec@1 98.828 (98.356)\n","Epoch 0: [120/196]\tTime 0.488 (0.489)\tData 0.001 (0.007)\tLoss 0.0333 (0.0501)\tPrec@1 99.219 (98.359)\n","Epoch 0: [130/196]\tTime 0.488 (0.489)\tData 0.002 (0.007)\tLoss 0.0503 (0.0502)\tPrec@1 98.828 (98.359)\n","Epoch 0: [140/196]\tTime 0.490 (0.489)\tData 0.002 (0.006)\tLoss 0.0456 (0.0500)\tPrec@1 98.438 (98.365)\n","Epoch 0: [150/196]\tTime 0.486 (0.489)\tData 0.001 (0.006)\tLoss 0.0579 (0.0499)\tPrec@1 97.656 (98.362)\n","Epoch 0: [160/196]\tTime 0.487 (0.488)\tData 0.001 (0.006)\tLoss 0.0497 (0.0503)\tPrec@1 98.438 (98.354)\n","Epoch 0: [170/196]\tTime 0.489 (0.488)\tData 0.001 (0.005)\tLoss 0.0609 (0.0504)\tPrec@1 98.047 (98.366)\n","Epoch 0: [180/196]\tTime 0.489 (0.488)\tData 0.002 (0.005)\tLoss 0.0384 (0.0502)\tPrec@1 98.438 (98.377)\n","Epoch 0: [190/196]\tTime 0.489 (0.488)\tData 0.001 (0.005)\tLoss 0.0981 (0.0505)\tPrec@1 96.875 (98.353)\n","Test: [10/100]\tTime 0.085 (0.128)\tLoss 0.2834 (0.2692)\tPrec@1 92.000 (93.500)\n","Test: [20/100]\tTime 0.088 (0.108)\tLoss 0.5065 (0.2719)\tPrec@1 90.000 (92.800)\n","Test: [30/100]\tTime 0.086 (0.101)\tLoss 0.1709 (0.2859)\tPrec@1 93.000 (92.133)\n","Test: [40/100]\tTime 0.088 (0.097)\tLoss 0.3846 (0.2966)\tPrec@1 91.000 (91.775)\n","Test: [50/100]\tTime 0.086 (0.095)\tLoss 0.4157 (0.3015)\tPrec@1 87.000 (91.600)\n","Test: [60/100]\tTime 0.087 (0.094)\tLoss 0.3204 (0.3004)\tPrec@1 90.000 (91.700)\n","Test: [70/100]\tTime 0.086 (0.093)\tLoss 0.3902 (0.2922)\tPrec@1 90.000 (91.829)\n","Test: [80/100]\tTime 0.087 (0.093)\tLoss 0.0953 (0.2885)\tPrec@1 95.000 (91.987)\n","Test: [90/100]\tTime 0.085 (0.092)\tLoss 0.4659 (0.2900)\tPrec@1 88.000 (91.933)\n","Test: [100/100]\tTime 0.087 (0.091)\tLoss 0.1870 (0.2856)\tPrec@1 93.000 (92.040)\n"," * Prec@1 92.040\n","Epoch 1: [10/196]\tTime 0.488 (0.545)\tData 0.000 (0.079)\tLoss 0.0676 (0.0463)\tPrec@1 97.656 (98.516)\n","Epoch 1: [20/196]\tTime 0.479 (0.515)\tData 0.002 (0.040)\tLoss 0.0222 (0.0408)\tPrec@1 99.609 (98.633)\n","Epoch 1: [30/196]\tTime 0.481 (0.505)\tData 0.001 (0.027)\tLoss 0.0465 (0.0401)\tPrec@1 98.828 (98.633)\n","Epoch 1: [40/196]\tTime 0.482 (0.500)\tData 0.002 (0.021)\tLoss 0.0220 (0.0385)\tPrec@1 99.219 (98.662)\n","Epoch 1: [50/196]\tTime 0.481 (0.497)\tData 0.001 (0.017)\tLoss 0.0380 (0.0392)\tPrec@1 98.438 (98.656)\n","Epoch 1: [60/196]\tTime 0.482 (0.495)\tData 0.002 (0.014)\tLoss 0.0239 (0.0374)\tPrec@1 98.828 (98.757)\n","Epoch 1: [70/196]\tTime 0.482 (0.493)\tData 0.002 (0.013)\tLoss 0.0311 (0.0361)\tPrec@1 99.609 (98.800)\n","Epoch 1: [80/196]\tTime 0.482 (0.492)\tData 0.002 (0.011)\tLoss 0.0361 (0.0363)\tPrec@1 98.828 (98.804)\n","Epoch 1: [90/196]\tTime 0.482 (0.491)\tData 0.002 (0.010)\tLoss 0.0621 (0.0366)\tPrec@1 97.656 (98.798)\n","Epoch 1: [100/196]\tTime 0.480 (0.491)\tData 0.001 (0.009)\tLoss 0.0249 (0.0359)\tPrec@1 98.438 (98.816)\n","Epoch 1: [110/196]\tTime 0.479 (0.490)\tData 0.002 (0.009)\tLoss 0.0386 (0.0351)\tPrec@1 99.219 (98.846)\n","Epoch 1: [120/196]\tTime 0.482 (0.489)\tData 0.001 (0.008)\tLoss 0.0421 (0.0344)\tPrec@1 98.828 (98.874)\n","Epoch 1: [130/196]\tTime 0.481 (0.489)\tData 0.001 (0.008)\tLoss 0.0270 (0.0339)\tPrec@1 99.219 (98.894)\n","Epoch 1: [140/196]\tTime 0.482 (0.489)\tData 0.001 (0.007)\tLoss 0.0351 (0.0335)\tPrec@1 98.438 (98.898)\n","Epoch 1: [150/196]\tTime 0.491 (0.489)\tData 0.002 (0.007)\tLoss 0.0146 (0.0336)\tPrec@1 100.000 (98.901)\n","Epoch 1: [160/196]\tTime 0.479 (0.488)\tData 0.001 (0.006)\tLoss 0.0434 (0.0338)\tPrec@1 98.828 (98.906)\n","Epoch 1: [170/196]\tTime 0.480 (0.488)\tData 0.002 (0.006)\tLoss 0.0267 (0.0333)\tPrec@1 99.219 (98.929)\n","Epoch 1: [180/196]\tTime 0.486 (0.488)\tData 0.002 (0.006)\tLoss 0.0327 (0.0332)\tPrec@1 98.438 (98.930)\n","Epoch 1: [190/196]\tTime 0.484 (0.488)\tData 0.001 (0.006)\tLoss 0.0251 (0.0330)\tPrec@1 98.828 (98.943)\n","Test: [10/100]\tTime 0.086 (0.134)\tLoss 0.2707 (0.2541)\tPrec@1 92.000 (93.600)\n","Test: [20/100]\tTime 0.087 (0.111)\tLoss 0.5270 (0.2685)\tPrec@1 91.000 (93.100)\n","Test: [30/100]\tTime 0.089 (0.103)\tLoss 0.2304 (0.2716)\tPrec@1 89.000 (92.833)\n","Test: [40/100]\tTime 0.087 (0.099)\tLoss 0.3268 (0.2772)\tPrec@1 92.000 (92.850)\n","Test: [50/100]\tTime 0.088 (0.097)\tLoss 0.4897 (0.2820)\tPrec@1 89.000 (92.720)\n","Test: [60/100]\tTime 0.087 (0.095)\tLoss 0.2891 (0.2752)\tPrec@1 92.000 (92.833)\n","Test: [70/100]\tTime 0.090 (0.094)\tLoss 0.3300 (0.2664)\tPrec@1 91.000 (92.971)\n","Test: [80/100]\tTime 0.087 (0.093)\tLoss 0.3044 (0.2709)\tPrec@1 89.000 (92.950)\n","Test: [90/100]\tTime 0.085 (0.093)\tLoss 0.3812 (0.2709)\tPrec@1 93.000 (92.900)\n","Test: [100/100]\tTime 0.085 (0.092)\tLoss 0.2122 (0.2695)\tPrec@1 92.000 (92.960)\n"," * Prec@1 92.960\n","Epoch 2: [10/196]\tTime 0.482 (0.543)\tData 0.000 (0.077)\tLoss 0.0129 (0.0233)\tPrec@1 99.609 (99.375)\n","Epoch 2: [20/196]\tTime 0.490 (0.515)\tData 0.001 (0.039)\tLoss 0.0090 (0.0198)\tPrec@1 100.000 (99.434)\n","Epoch 2: [30/196]\tTime 0.480 (0.505)\tData 0.001 (0.027)\tLoss 0.0445 (0.0211)\tPrec@1 98.438 (99.375)\n","Epoch 2: [40/196]\tTime 0.482 (0.500)\tData 0.001 (0.020)\tLoss 0.0273 (0.0216)\tPrec@1 99.219 (99.355)\n","Epoch 2: [50/196]\tTime 0.484 (0.497)\tData 0.002 (0.017)\tLoss 0.0212 (0.0209)\tPrec@1 99.609 (99.383)\n","Epoch 2: [60/196]\tTime 0.482 (0.495)\tData 0.001 (0.014)\tLoss 0.0184 (0.0213)\tPrec@1 99.219 (99.362)\n","Epoch 2: [70/196]\tTime 0.483 (0.494)\tData 0.002 (0.012)\tLoss 0.0060 (0.0210)\tPrec@1 100.000 (99.364)\n","Epoch 2: [80/196]\tTime 0.481 (0.493)\tData 0.002 (0.011)\tLoss 0.0176 (0.0208)\tPrec@1 99.219 (99.370)\n","Epoch 2: [90/196]\tTime 0.476 (0.492)\tData 0.002 (0.010)\tLoss 0.0076 (0.0205)\tPrec@1 100.000 (99.388)\n","Epoch 2: [100/196]\tTime 0.486 (0.491)\tData 0.001 (0.009)\tLoss 0.0224 (0.0205)\tPrec@1 99.609 (99.387)\n","Epoch 2: [110/196]\tTime 0.481 (0.491)\tData 0.001 (0.009)\tLoss 0.0181 (0.0205)\tPrec@1 99.609 (99.371)\n","Epoch 2: [120/196]\tTime 0.475 (0.490)\tData 0.002 (0.008)\tLoss 0.0112 (0.0204)\tPrec@1 100.000 (99.385)\n","Epoch 2: [130/196]\tTime 0.484 (0.490)\tData 0.002 (0.007)\tLoss 0.0187 (0.0202)\tPrec@1 99.609 (99.396)\n","Epoch 2: [140/196]\tTime 0.485 (0.489)\tData 0.002 (0.007)\tLoss 0.0280 (0.0199)\tPrec@1 98.828 (99.408)\n","Epoch 2: [150/196]\tTime 0.483 (0.489)\tData 0.002 (0.007)\tLoss 0.0378 (0.0206)\tPrec@1 98.828 (99.404)\n","Epoch 2: [160/196]\tTime 0.482 (0.489)\tData 0.001 (0.006)\tLoss 0.0169 (0.0206)\tPrec@1 99.609 (99.402)\n","Epoch 2: [170/196]\tTime 0.480 (0.489)\tData 0.002 (0.006)\tLoss 0.0145 (0.0204)\tPrec@1 99.609 (99.405)\n","Epoch 2: [180/196]\tTime 0.476 (0.488)\tData 0.002 (0.006)\tLoss 0.0124 (0.0202)\tPrec@1 100.000 (99.410)\n","Epoch 2: [190/196]\tTime 0.483 (0.488)\tData 0.001 (0.006)\tLoss 0.0082 (0.0200)\tPrec@1 100.000 (99.414)\n","Test: [10/100]\tTime 0.087 (0.136)\tLoss 0.2566 (0.2319)\tPrec@1 93.000 (94.000)\n","Test: [20/100]\tTime 0.087 (0.111)\tLoss 0.4898 (0.2383)\tPrec@1 93.000 (93.950)\n","Test: [30/100]\tTime 0.087 (0.103)\tLoss 0.0988 (0.2451)\tPrec@1 95.000 (93.700)\n","Test: [40/100]\tTime 0.086 (0.099)\tLoss 0.1766 (0.2497)\tPrec@1 96.000 (93.450)\n","Test: [50/100]\tTime 0.090 (0.097)\tLoss 0.4026 (0.2555)\tPrec@1 87.000 (93.300)\n","Test: [60/100]\tTime 0.088 (0.095)\tLoss 0.3114 (0.2484)\tPrec@1 89.000 (93.433)\n","Test: [70/100]\tTime 0.086 (0.094)\tLoss 0.2811 (0.2397)\tPrec@1 91.000 (93.529)\n","Test: [80/100]\tTime 0.086 (0.093)\tLoss 0.2271 (0.2440)\tPrec@1 91.000 (93.425)\n","Test: [90/100]\tTime 0.086 (0.093)\tLoss 0.3315 (0.2429)\tPrec@1 94.000 (93.411)\n","Test: [100/100]\tTime 0.085 (0.092)\tLoss 0.1718 (0.2404)\tPrec@1 94.000 (93.460)\n"," * Prec@1 93.460\n","Epoch 3: [10/196]\tTime 0.502 (0.549)\tData 0.000 (0.079)\tLoss 0.0057 (0.0134)\tPrec@1 100.000 (99.766)\n","Epoch 3: [20/196]\tTime 0.487 (0.517)\tData 0.001 (0.040)\tLoss 0.0257 (0.0144)\tPrec@1 98.438 (99.648)\n","Epoch 3: [30/196]\tTime 0.487 (0.506)\tData 0.002 (0.027)\tLoss 0.0122 (0.0140)\tPrec@1 99.219 (99.635)\n","Epoch 3: [40/196]\tTime 0.479 (0.501)\tData 0.002 (0.021)\tLoss 0.0113 (0.0142)\tPrec@1 99.609 (99.629)\n","Epoch 3: [50/196]\tTime 0.491 (0.498)\tData 0.001 (0.017)\tLoss 0.0444 (0.0155)\tPrec@1 98.828 (99.594)\n","Epoch 3: [60/196]\tTime 0.488 (0.496)\tData 0.001 (0.014)\tLoss 0.0066 (0.0153)\tPrec@1 100.000 (99.590)\n","Epoch 3: [70/196]\tTime 0.488 (0.494)\tData 0.002 (0.013)\tLoss 0.0078 (0.0160)\tPrec@1 100.000 (99.581)\n","Epoch 3: [80/196]\tTime 0.491 (0.493)\tData 0.002 (0.011)\tLoss 0.0116 (0.0154)\tPrec@1 100.000 (99.604)\n","Epoch 3: [90/196]\tTime 0.486 (0.492)\tData 0.002 (0.010)\tLoss 0.0107 (0.0155)\tPrec@1 100.000 (99.596)\n","Epoch 3: [100/196]\tTime 0.486 (0.491)\tData 0.001 (0.009)\tLoss 0.0204 (0.0155)\tPrec@1 99.219 (99.594)\n","Epoch 3: [110/196]\tTime 0.490 (0.490)\tData 0.002 (0.009)\tLoss 0.0304 (0.0157)\tPrec@1 98.438 (99.581)\n","Epoch 3: [120/196]\tTime 0.490 (0.490)\tData 0.002 (0.008)\tLoss 0.0134 (0.0157)\tPrec@1 100.000 (99.593)\n","Epoch 3: [130/196]\tTime 0.487 (0.490)\tData 0.001 (0.008)\tLoss 0.0085 (0.0158)\tPrec@1 100.000 (99.591)\n","Epoch 3: [140/196]\tTime 0.489 (0.489)\tData 0.002 (0.007)\tLoss 0.0287 (0.0159)\tPrec@1 98.438 (99.576)\n","Epoch 3: [150/196]\tTime 0.488 (0.489)\tData 0.001 (0.007)\tLoss 0.0234 (0.0158)\tPrec@1 99.609 (99.573)\n","Epoch 3: [160/196]\tTime 0.490 (0.489)\tData 0.001 (0.006)\tLoss 0.0124 (0.0159)\tPrec@1 99.609 (99.561)\n","Epoch 3: [170/196]\tTime 0.481 (0.488)\tData 0.002 (0.006)\tLoss 0.0189 (0.0158)\tPrec@1 99.609 (99.566)\n","Epoch 3: [180/196]\tTime 0.488 (0.488)\tData 0.002 (0.006)\tLoss 0.0052 (0.0160)\tPrec@1 100.000 (99.564)\n","Epoch 3: [190/196]\tTime 0.489 (0.488)\tData 0.001 (0.006)\tLoss 0.0128 (0.0157)\tPrec@1 100.000 (99.579)\n","Test: [10/100]\tTime 0.087 (0.129)\tLoss 0.2150 (0.2182)\tPrec@1 92.000 (93.600)\n","Test: [20/100]\tTime 0.089 (0.109)\tLoss 0.4133 (0.2341)\tPrec@1 92.000 (93.250)\n","Test: [30/100]\tTime 0.088 (0.101)\tLoss 0.0962 (0.2346)\tPrec@1 94.000 (93.333)\n","Test: [40/100]\tTime 0.088 (0.098)\tLoss 0.2325 (0.2427)\tPrec@1 93.000 (93.225)\n","Test: [50/100]\tTime 0.086 (0.096)\tLoss 0.4171 (0.2492)\tPrec@1 91.000 (93.180)\n","Test: [60/100]\tTime 0.090 (0.094)\tLoss 0.2351 (0.2410)\tPrec@1 92.000 (93.367)\n","Test: [70/100]\tTime 0.088 (0.093)\tLoss 0.2093 (0.2347)\tPrec@1 91.000 (93.471)\n","Test: [80/100]\tTime 0.089 (0.093)\tLoss 0.1104 (0.2369)\tPrec@1 94.000 (93.438)\n","Test: [90/100]\tTime 0.085 (0.092)\tLoss 0.2671 (0.2349)\tPrec@1 95.000 (93.511)\n","Test: [100/100]\tTime 0.085 (0.091)\tLoss 0.1385 (0.2346)\tPrec@1 95.000 (93.530)\n"," * Prec@1 93.530\n","Epoch 4: [10/196]\tTime 0.489 (0.546)\tData 0.000 (0.079)\tLoss 0.0045 (0.0115)\tPrec@1 100.000 (99.727)\n","Epoch 4: [20/196]\tTime 0.495 (0.516)\tData 0.002 (0.040)\tLoss 0.0109 (0.0125)\tPrec@1 100.000 (99.707)\n","Epoch 4: [30/196]\tTime 0.491 (0.506)\tData 0.001 (0.027)\tLoss 0.0168 (0.0129)\tPrec@1 99.219 (99.648)\n","Epoch 4: [40/196]\tTime 0.489 (0.501)\tData 0.001 (0.021)\tLoss 0.0374 (0.0139)\tPrec@1 99.219 (99.639)\n","Epoch 4: [50/196]\tTime 0.491 (0.498)\tData 0.001 (0.017)\tLoss 0.0106 (0.0134)\tPrec@1 99.609 (99.672)\n","Epoch 4: [60/196]\tTime 0.491 (0.496)\tData 0.002 (0.015)\tLoss 0.0062 (0.0131)\tPrec@1 100.000 (99.681)\n","Epoch 4: [70/196]\tTime 0.489 (0.494)\tData 0.002 (0.013)\tLoss 0.0123 (0.0131)\tPrec@1 99.609 (99.676)\n","Epoch 4: [80/196]\tTime 0.487 (0.493)\tData 0.001 (0.011)\tLoss 0.0081 (0.0129)\tPrec@1 100.000 (99.678)\n","Epoch 4: [90/196]\tTime 0.487 (0.492)\tData 0.002 (0.010)\tLoss 0.0324 (0.0135)\tPrec@1 98.828 (99.657)\n","Epoch 4: [100/196]\tTime 0.487 (0.491)\tData 0.001 (0.009)\tLoss 0.0076 (0.0134)\tPrec@1 99.609 (99.664)\n","Epoch 4: [110/196]\tTime 0.485 (0.491)\tData 0.001 (0.009)\tLoss 0.0136 (0.0130)\tPrec@1 99.609 (99.677)\n","Epoch 4: [120/196]\tTime 0.490 (0.490)\tData 0.001 (0.008)\tLoss 0.0277 (0.0132)\tPrec@1 99.219 (99.671)\n","Epoch 4: [130/196]\tTime 0.486 (0.490)\tData 0.001 (0.008)\tLoss 0.0134 (0.0137)\tPrec@1 99.219 (99.651)\n","Epoch 4: [140/196]\tTime 0.491 (0.490)\tData 0.002 (0.007)\tLoss 0.0115 (0.0138)\tPrec@1 99.609 (99.646)\n","Epoch 4: [150/196]\tTime 0.487 (0.489)\tData 0.001 (0.007)\tLoss 0.0265 (0.0137)\tPrec@1 99.609 (99.659)\n","Epoch 4: [160/196]\tTime 0.488 (0.489)\tData 0.002 (0.006)\tLoss 0.0099 (0.0136)\tPrec@1 99.609 (99.658)\n","Epoch 4: [170/196]\tTime 0.488 (0.489)\tData 0.002 (0.006)\tLoss 0.0136 (0.0134)\tPrec@1 99.609 (99.669)\n","Epoch 4: [180/196]\tTime 0.485 (0.489)\tData 0.002 (0.006)\tLoss 0.0138 (0.0135)\tPrec@1 100.000 (99.670)\n","Epoch 4: [190/196]\tTime 0.488 (0.489)\tData 0.001 (0.006)\tLoss 0.0140 (0.0136)\tPrec@1 99.609 (99.665)\n","Test: [10/100]\tTime 0.086 (0.135)\tLoss 0.2537 (0.2326)\tPrec@1 93.000 (94.000)\n","Test: [20/100]\tTime 0.086 (0.111)\tLoss 0.4012 (0.2477)\tPrec@1 94.000 (93.600)\n","Test: [30/100]\tTime 0.087 (0.103)\tLoss 0.1659 (0.2450)\tPrec@1 96.000 (93.567)\n","Test: [40/100]\tTime 0.088 (0.100)\tLoss 0.2214 (0.2446)\tPrec@1 94.000 (93.375)\n","Test: [50/100]\tTime 0.088 (0.097)\tLoss 0.4965 (0.2513)\tPrec@1 90.000 (93.380)\n","Test: [60/100]\tTime 0.087 (0.095)\tLoss 0.2737 (0.2454)\tPrec@1 89.000 (93.483)\n","Test: [70/100]\tTime 0.086 (0.094)\tLoss 0.2293 (0.2391)\tPrec@1 92.000 (93.643)\n","Test: [80/100]\tTime 0.087 (0.093)\tLoss 0.1769 (0.2434)\tPrec@1 92.000 (93.638)\n","Test: [90/100]\tTime 0.087 (0.093)\tLoss 0.2516 (0.2402)\tPrec@1 94.000 (93.644)\n","Test: [100/100]\tTime 0.085 (0.092)\tLoss 0.1331 (0.2377)\tPrec@1 94.000 (93.640)\n"," * Prec@1 93.640\n","==> Fine-tuned model accuracy\n","Test: [10/100]\tTime 0.090 (0.136)\tLoss 0.2537 (0.2326)\tPrec@1 93.000 (94.000)\n","Test: [20/100]\tTime 0.087 (0.111)\tLoss 0.4012 (0.2477)\tPrec@1 94.000 (93.600)\n","Test: [30/100]\tTime 0.086 (0.103)\tLoss 0.1659 (0.2450)\tPrec@1 96.000 (93.567)\n","Test: [40/100]\tTime 0.088 (0.099)\tLoss 0.2214 (0.2446)\tPrec@1 94.000 (93.375)\n","Test: [50/100]\tTime 0.088 (0.097)\tLoss 0.4965 (0.2513)\tPrec@1 90.000 (93.380)\n","Test: [60/100]\tTime 0.086 (0.095)\tLoss 0.2737 (0.2454)\tPrec@1 89.000 (93.483)\n","Test: [70/100]\tTime 0.088 (0.094)\tLoss 0.2293 (0.2391)\tPrec@1 92.000 (93.643)\n","Test: [80/100]\tTime 0.087 (0.093)\tLoss 0.1769 (0.2434)\tPrec@1 92.000 (93.638)\n","Test: [90/100]\tTime 0.085 (0.092)\tLoss 0.2516 (0.2402)\tPrec@1 94.000 (93.644)\n","Test: [100/100]\tTime 0.085 (0.092)\tLoss 0.1331 (0.2377)\tPrec@1 94.000 (93.640)\n"," * Prec@1 93.640\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["93.64"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"SNOhbmiSTmBs","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}